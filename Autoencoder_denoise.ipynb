{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Autoencoder denoiser**\n",
        "\n",
        "### A CNN to denoise corrupted images based on Fashion MNIST dataset.\n",
        "\n",
        "Autoencoder architecture provides an encoded latent space of reduced features which can be decoded back to its original form.\n",
        "\n",
        "Autoencoder is an unsupervised methodology similar, but more flexible than Principal Component Analysis (PCA). It doesn't resemble the probabilistic distribution unlike Variational Auto Encoders."
      ],
      "metadata": {
        "id": "J-I_i7hwyTIY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SjMbdf6xxsa"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Dataset"
      ],
      "metadata": {
        "id": "fscaeufhyt-o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Load the dataset\n",
        "In this case we are loading dataset separately for train and test dataset.\n",
        "\n",
        "We include an artificial noise to the image."
      ],
      "metadata": {
        "id": "l1HGe8awyyhh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset, test_dataset = tfds.load('fashion_mnist', as_supervised=True, split=['train', 'test'])"
      ],
      "metadata": {
        "id": "1sYONe_IzTSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Normalize and add noise\n",
        "- Normalize the values to the range of 0-1\n",
        "- Intentionally corrupt the dataset with random noise.\n",
        "- We want a grainy pictures, not a slight noise.\n",
        "- **Important: we replace the classification labels with original images. We want to compare images instead of classifying them.**"
      ],
      "metadata": {
        "id": "eUYyTg_tzg7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_noise(image, label):\n",
        "    image = tf.cast(image, dtype=tf.float32)\n",
        "    image /= 255.0\n",
        "\n",
        "    noise = tf.random.normal(shape=image.shape)\n",
        "    noise *= 0.5\n",
        "\n",
        "    noise_image = image + noise\n",
        "    noise_image = tf.clip_by_value(noise_image, 0.0, 1.0)\n",
        "\n",
        "    return noise_image, image"
      ],
      "metadata": {
        "id": "_3B_0suhzj3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Process the images in datasets"
      ],
      "metadata": {
        "id": "vVRW-lOG0dOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 128\n",
        "SHUFFLE_BUFFER_SIZE = 1024\n",
        "\n",
        "train_dataset = train_dataset.map(add_noise)\n",
        "test_dataset = test_dataset.map(add_noise)\n",
        "\n",
        "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE).repeat()"
      ],
      "metadata": {
        "id": "r5r5xAn00ZSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4 Preview the noisy images\n",
        "*By the way -  brain processing according to gestalt principles and top-down visual perception mostly makes it trivial to guess what's on the picture - but some of them doesn't look like they contain anything at all. It's fascinating how AI can handle that.*"
      ],
      "metadata": {
        "id": "a6CPyV4IleBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "row = 1\n",
        "for image, label in train_dataset.take(10):\n",
        "    plt.subplot(1, 10, row)\n",
        "    plt.imshow(image[0], cmap='gray')\n",
        "    plt.axis('off')\n",
        "    row += 1"
      ],
      "metadata": {
        "id": "bNhbUqe5dVEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Create the Model\n",
        "\n",
        "**Autoencoder consists of 3 building blocks:**\n",
        "- **Encoder:** Reduce dimensions to low-dimensional latent space.\n",
        "- **Bottleneck:** The most compressed representation of our data.\n",
        "- **Decoder:** Decodes the image from it's compressed form.\n",
        "\n",
        "Instead of sequential or functional API, these building blocks are made as classes."
      ],
      "metadata": {
        "id": "JaqJOa_O1B4Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Encoder"
      ],
      "metadata": {
        "id": "56SjLqVGekp0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.conv_1 = tf.keras.layers.Conv2D(filters=64, \n",
        "                                             kernel_size=(3,3), \n",
        "                                             activation='relu', \n",
        "                                             padding='same')\n",
        "\n",
        "        self.maxpool_1 = tf.keras.layers.MaxPooling2D(pool_size=(2,2))\n",
        "\n",
        "        self.conv_2 = tf.keras.layers.Conv2D(filters=128, \n",
        "                                             kernel_size=(3,3), \n",
        "                                             activation='relu', \n",
        "                                             padding='same')\n",
        "\n",
        "        self.maxpool_2 = tf.keras.layers.MaxPooling2D(pool_size=(2,2))\n",
        "\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.conv_1(inputs)\n",
        "        x = self.maxpool_1(x)\n",
        "        x = self.conv_2(x)\n",
        "        x = self.maxpool_2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "KQeJ2hBJercs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Bottleneck"
      ],
      "metadata": {
        "id": "krrrf-HbesX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Bottleneck(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.bottleneck = tf.keras.layers.Conv2D(filters=256, \n",
        "                                                 kernel_size=(3,3), \n",
        "                                                 activation='relu', \n",
        "                                                 padding='same')\n",
        "\n",
        "        self.encoder_visualization = tf.keras.layers.Conv2D(filters=1, \n",
        "                                                            kernel_size=(3,3), \n",
        "                                                            name='encoder_visualization',\n",
        "                                                            activation='sigmoid', \n",
        "                                                            trainable=False,\n",
        "                                                            padding='same')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.bottleneck(inputs)\n",
        "        vis = self.encoder_visualization(x)\n",
        "        return x, vis"
      ],
      "metadata": {
        "id": "UvJeIEKwevaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Decoder"
      ],
      "metadata": {
        "id": "y4th5O0ze2zc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.conv_1 = tf.keras.layers.Conv2D(filters=128, \n",
        "                                             kernel_size=(3,3), \n",
        "                                             activation='relu', \n",
        "                                             padding='same')\n",
        "        \n",
        "        self.upsample_1 = tf.keras.layers.UpSampling2D(size=(2,2))\n",
        "\n",
        "        self.conv_2 = tf.keras.layers.Conv2D(filters=64, \n",
        "                                             kernel_size=(3,3), \n",
        "                                             activation='relu', \n",
        "                                             padding='same')\n",
        "        \n",
        "        self.upsample_2 = tf.keras.layers.UpSampling2D(size=(2,2))\n",
        "\n",
        "        self.conv_3 = tf.keras.layers.Conv2D(filters=1, \n",
        "                                             kernel_size=(3,3), \n",
        "                                             activation='sigmoid', \n",
        "                                             padding='same')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.conv_1(inputs)\n",
        "        x = self.upsample_1(x)\n",
        "        x = self.conv_2(x)\n",
        "        x = self.upsample_2(x)  \n",
        "        x = self.conv_3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "7bVEgANRe9VP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4 Autoencoder model\n",
        "Merge each subclasses into the final Autoencoder model.\n",
        "\n",
        "**Keras subclass API provides great flexibility but differers from functional and sequential API:**\n",
        "- You can customize every function but bugs are harder to track.\n",
        "- To build the model you have to pass it's initial input shape `RAW_INPUT_SHAPE`\n",
        "- Model summary doesn't provide the output sizes unless you create a customized summary function which passes the input shape and then calls summary.\n",
        "\n",
        "*Autoencoder might be too basic for this kind of development because of many boilerplate code necessary.*\n",
        "\n",
        "**TODO: Custom compile and training loop**\n"
      ],
      "metadata": {
        "id": "RqbPp8AkhjK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AutoEncoder(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(AutoEncoder, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder()\n",
        "        self.bottleneck = Bottleneck()\n",
        "        self.decoder = Decoder()\n",
        "\n",
        "\n",
        "    def call(self, inputs):\n",
        "\n",
        "        x = self.encoder(inputs)\n",
        "        x, _ = self.bottleneck(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "    def summary(self, shape):\n",
        "        x = tf.keras.layers.Input(shape=shape)\n",
        "        temp_model = tf.keras.Model(inputs=x, outputs=self.call(x))\n",
        "        return temp_model.summary()\n",
        "\n",
        "\n",
        "INPUT_SHAPE = (28,28,1)\n",
        "RAW_INPUT_SHAPE = (1, *INPUT_SHAPE)\n",
        "\n",
        "denoise_model = AutoEncoder()\n",
        "denoise_model.build(RAW_INPUT_SHAPE)\n",
        "denoise_model.summary(INPUT_SHAPE)\n",
        "\n",
        "# denoise_model.encoder.summary()\n",
        "# denoise_model.bottleneck.summary()\n",
        "# denoise_model.decoder.summary()"
      ],
      "metadata": {
        "id": "-PCWCa-XeEOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Compile and train"
      ],
      "metadata": {
        "id": "LyGOd8_F2Ao3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_STEPS = 60000 // BATCH_SIZE\n",
        "VALID_STEPS = 60000 // BATCH_SIZE\n",
        "EPOCHS = 40\n",
        "\n",
        "denoise_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(), \n",
        "    loss=tf.keras.losses.BinaryCrossentropy()\n",
        ")\n",
        "\n",
        "denoise_model_history = denoise_model.fit(train_dataset, steps_per_epoch=TRAIN_STEPS, validation_data=test_dataset, validation_steps=VALID_STEPS, epochs=EPOCHS)"
      ],
      "metadata": {
        "id": "HkT4Q1B87ZCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Results"
      ],
      "metadata": {
        "id": "W9N6Ypq2lZOS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Visualization helper model\n",
        "A helper model to reach the most compressed input representation inside the bottleneck. It's a subclass of Autoencoder but returns the helper layer instead of passing data to decoder."
      ],
      "metadata": {
        "id": "LD0BJthElgtW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder_visualization(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(Encoder_visualization, self).__init__()\n",
        "        self.encoder = denoise_model.encoder\n",
        "        self.bottleneck = denoise_model.bottleneck\n",
        "    \n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.encoder(inputs)\n",
        "        _, vis = self.bottleneck(x)\n",
        "\n",
        "        return vis\n",
        "\n",
        "\n",
        "encoder_visualization_model = Encoder_visualization()\n",
        "encoder_visualization_model.build(RAW_INPUT_SHAPE)\n",
        "encoder_visualization_model.summary()"
      ],
      "metadata": {
        "id": "K5l6PRjB2eKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Display denoising result"
      ],
      "metadata": {
        "id": "LZIGAiqcllf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_one_row(disp_images, offset, shape):\n",
        "    for index, noisy_image in enumerate(disp_images):\n",
        "        plt.subplot(3, 10, offset + index + 1)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        noisy_image = np.reshape(noisy_image, shape)\n",
        "        plt.imshow(noisy_image, cmap='gray')\n",
        "\n",
        "\n",
        "def display_results(disp_input_images, disp_encoded, disp_predicted, enc_shape):\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    display_one_row(disp_input_images, 0, shape=(28,28))\n",
        "    display_one_row(disp_encoded, 10, shape=enc_shape)\n",
        "    display_one_row(disp_predicted, 20, shape=(28,28))"
      ],
      "metadata": {
        "id": "q6wBpXKJ2OXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEST_BATCH = test_dataset.take(1)\n",
        "PREVIEW_AMOUNT = 10\n",
        "INDEX = np.random.randint(0, BATCH_SIZE, size=PREVIEW_AMOUNT)\n",
        "\n",
        "output_samples = []\n",
        "for input_image, image in tfds.as_numpy(TEST_BATCH):\n",
        "    output_samples = input_image\n",
        "\n",
        "output_samples = np.array(output_samples[INDEX])\n",
        "\n",
        "encoded = encoder_visualization_model.predict(output_samples)\n",
        "predicted = denoise_model.predict(output_samples)\n",
        "\n",
        "display_results(output_samples, encoded, predicted, enc_shape=(7,7))"
      ],
      "metadata": {
        "id": "lGSSTGSG_wV-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}